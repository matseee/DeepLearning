{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "\n",
    "images, labels = (xTrain[0:1000].reshape(1000, 28*28) / 255, yTrain[0:1000])\n",
    "\n",
    "oneHotLabels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i,l in enumerate(labels):\n",
    "    oneHotLabels[i][l] = 1\n",
    "\n",
    "labels = oneHotLabels\n",
    "\n",
    "testImages = xTest.reshape(len(xTest), 28*28) / 255\n",
    "testLabels = np.zeros((len(yTest), 10))\n",
    "\n",
    "for i,l in enumerate(yTest):\n",
    "    testLabels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "relu = lambda x: (x>=0) * x\n",
    "relu2Deriv = lambda x: x>=0\n",
    "\n",
    "alpha = 0.005\n",
    "iterations = 350\n",
    "hiddenSize = 40\n",
    "pixelsPerImage = 784\n",
    "numLabels = 10\n",
    "\n",
    "weights1 = 0.2 * np.random.random((pixelsPerImage, hiddenSize)) - 0.1\n",
    "weights2 = 0.2 * np.random.random((hiddenSize, numLabels)) - 0.1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    trainingError, trainingCorrectCount = (0.0, 0)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        layer0 = images[i:i+1]\n",
    "        layer1 = relu(np.dot(layer0, weights1))\n",
    "        layer2 = np.dot(layer1, weights2)\n",
    "\n",
    "        trainingError += np.sum((layer2 - labels[i:i+1]) ** 2)\n",
    "        trainingCorrectCount += int(np.argmax(layer2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        delta2 = layer2 - labels[i:i+1]\n",
    "        delta1 = delta2.dot(weights2.T) * relu2Deriv(layer1)\n",
    "\n",
    "        weights2 -= alpha * layer1.T.dot(delta2)\n",
    "        weights1 -= alpha * layer0.T.dot(delta1)\n",
    "    \n",
    "    if iter % 10 == 0 or iter == iterations-1:\n",
    "        testError, testCorrectCount = (0.0, 0)\n",
    "\n",
    "        for i in range(len(testImages)):\n",
    "            layer0 = testImages[i:i+1]\n",
    "            layer1 = relu(np.dot(layer0, weights1))\n",
    "            layer2 = np.dot(layer1, weights2)\n",
    "\n",
    "            testError += np.sum((layer2 - testLabels[i:i+1]) ** 2)\n",
    "            testCorrectCount += int(np.argmax(layer2) == np.argmax(testLabels[i:i+1]))\n",
    "\n",
    "        sys.stdout.write(\"Test-Error:\" + str(testError/float(len(testImages)))[0:5] + \" Test-KKR:\" + str(testCorrectCount/float(len(testImages))))\n",
    "        sys.stdout.write(\" Training-Error:\" + str(trainingError/float(len(images)))[0:5] + \" Training-KKR:\" + str(trainingCorrectCount/float(len(images))))\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test-Error:0.601 Test-KKR:0.6488 Training-Error:0.722 Training-KKR:0.537\n",
      "Test-Error:0.420 Test-KKR:0.8114 Training-Error:0.312 Training-KKR:0.901\n",
      "Test-Error:0.414 Test-KKR:0.8111 Training-Error:0.260 Training-KKR:0.93\n",
      "Test-Error:0.417 Test-KKR:0.8066 Training-Error:0.232 Training-KKR:0.946\n",
      "Test-Error:0.426 Test-KKR:0.8019 Training-Error:0.215 Training-KKR:0.956\n",
      "Test-Error:0.437 Test-KKR:0.7982 Training-Error:0.204 Training-KKR:0.966\n",
      "Test-Error:0.448 Test-KKR:0.7921 Training-Error:0.194 Training-KKR:0.967\n",
      "Test-Error:0.458 Test-KKR:0.7864 Training-Error:0.186 Training-KKR:0.975\n",
      "Test-Error:0.466 Test-KKR:0.7817 Training-Error:0.179 Training-KKR:0.979\n",
      "Test-Error:0.474 Test-KKR:0.7758 Training-Error:0.172 Training-KKR:0.981\n",
      "Test-Error:0.482 Test-KKR:0.7706 Training-Error:0.166 Training-KKR:0.984\n",
      "Test-Error:0.489 Test-KKR:0.7686 Training-Error:0.161 Training-KKR:0.984\n",
      "Test-Error:0.496 Test-KKR:0.766 Training-Error:0.157 Training-KKR:0.986\n",
      "Test-Error:0.502 Test-KKR:0.7622 Training-Error:0.153 Training-KKR:0.99\n",
      "Test-Error:0.508 Test-KKR:0.758 Training-Error:0.149 Training-KKR:0.991\n",
      "Test-Error:0.513 Test-KKR:0.7558 Training-Error:0.145 Training-KKR:0.991\n",
      "Test-Error:0.518 Test-KKR:0.7553 Training-Error:0.141 Training-KKR:0.992\n",
      "Test-Error:0.524 Test-KKR:0.751 Training-Error:0.138 Training-KKR:0.992\n",
      "Test-Error:0.528 Test-KKR:0.7505 Training-Error:0.135 Training-KKR:0.995\n",
      "Test-Error:0.533 Test-KKR:0.7482 Training-Error:0.132 Training-KKR:0.995\n",
      "Test-Error:0.538 Test-KKR:0.7464 Training-Error:0.130 Training-KKR:0.998\n",
      "Test-Error:0.544 Test-KKR:0.7446 Training-Error:0.127 Training-KKR:0.998\n",
      "Test-Error:0.552 Test-KKR:0.7416 Training-Error:0.125 Training-KKR:0.998\n",
      "Test-Error:0.560 Test-KKR:0.7372 Training-Error:0.123 Training-KKR:0.998\n",
      "Test-Error:0.569 Test-KKR:0.7344 Training-Error:0.121 Training-KKR:0.998\n",
      "Test-Error:0.577 Test-KKR:0.7316 Training-Error:0.120 Training-KKR:0.999\n",
      "Test-Error:0.585 Test-KKR:0.729 Training-Error:0.118 Training-KKR:0.999\n",
      "Test-Error:0.593 Test-KKR:0.7259 Training-Error:0.117 Training-KKR:0.999\n",
      "Test-Error:0.600 Test-KKR:0.723 Training-Error:0.115 Training-KKR:0.999\n",
      "Test-Error:0.607 Test-KKR:0.7196 Training-Error:0.114 Training-KKR:0.999\n",
      "Test-Error:0.614 Test-KKR:0.7183 Training-Error:0.113 Training-KKR:0.999\n",
      "Test-Error:0.622 Test-KKR:0.7165 Training-Error:0.112 Training-KKR:0.999\n",
      "Test-Error:0.629 Test-KKR:0.7133 Training-Error:0.111 Training-KKR:0.999\n",
      "Test-Error:0.637 Test-KKR:0.7125 Training-Error:0.110 Training-KKR:0.999\n",
      "Test-Error:0.645 Test-KKR:0.71 Training-Error:0.109 Training-KKR:1.0\n",
      "Test-Error:0.653 Test-KKR:0.7073 Training-Error:0.108 Training-KKR:1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# WITH RIGHT RELU FUNCTION => ITS WRONG ABOVE AND IN THE BOOK\n",
    "import sys, numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "\n",
    "images, labels = (xTrain[0:1000].reshape(1000, 28*28) / 255, yTrain[0:1000])\n",
    "\n",
    "oneHotLabels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i,l in enumerate(labels):\n",
    "    oneHotLabels[i][l] = 1\n",
    "\n",
    "labels = oneHotLabels\n",
    "\n",
    "testImages = xTest.reshape(len(xTest), 28*28) / 255\n",
    "testLabels = np.zeros((len(yTest), 10))\n",
    "\n",
    "for i,l in enumerate(yTest):\n",
    "    testLabels[i][l] = 1\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "relu = lambda x: (x>0) * x\n",
    "relu2Deriv = lambda x: x>0\n",
    "\n",
    "alpha = 0.005\n",
    "iterations = 350\n",
    "hiddenSize = 40\n",
    "pixelsPerImage = 784\n",
    "numLabels = 10\n",
    "\n",
    "weights1 = 0.2 * np.random.random((pixelsPerImage, hiddenSize)) - 0.1\n",
    "weights2 = 0.2 * np.random.random((hiddenSize, numLabels)) - 0.1\n",
    "\n",
    "for iter in range(iterations):\n",
    "    trainingError, trainingCorrectCount = (0.0, 0)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        layer0 = images[i:i+1]\n",
    "        layer1 = relu(np.dot(layer0, weights1))\n",
    "        layer2 = np.dot(layer1, weights2)\n",
    "\n",
    "        trainingError += np.sum((layer2 - labels[i:i+1]) ** 2)\n",
    "        trainingCorrectCount += int(np.argmax(layer2) == np.argmax(labels[i:i+1]))\n",
    "\n",
    "        delta2 = layer2 - labels[i:i+1]\n",
    "        delta1 = delta2.dot(weights2.T) * relu2Deriv(layer1)\n",
    "\n",
    "        weights2 -= alpha * layer1.T.dot(delta2)\n",
    "        weights1 -= alpha * layer0.T.dot(delta1)\n",
    "    \n",
    "    if iter % 10 == 0 or iter == iterations-1:\n",
    "        testError, testCorrectCount = (0.0, 0)\n",
    "\n",
    "        for i in range(len(testImages)):\n",
    "            layer0 = testImages[i:i+1]\n",
    "            layer1 = relu(np.dot(layer0, weights1))\n",
    "            layer2 = np.dot(layer1, weights2)\n",
    "\n",
    "            testError += np.sum((layer2 - testLabels[i:i+1]) ** 2)\n",
    "            testCorrectCount += int(np.argmax(layer2) == np.argmax(testLabels[i:i+1]))\n",
    "\n",
    "        sys.stdout.write(\"Test-Error:\" + str(testError/float(len(testImages)))[0:5] + \" Test-KKR:\" + str(testCorrectCount/float(len(testImages))))\n",
    "        sys.stdout.write(\" Training-Error:\" + str(trainingError/float(len(images)))[0:5] + \" Training-KKR:\" + str(trainingCorrectCount/float(len(images))))\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test-Error:0.593 Test-KKR:0.6493 Training-Error:0.729 Training-KKR:0.527\n",
      "Test-Error:0.321 Test-KKR:0.8559 Training-Error:0.198 Training-KKR:0.94\n",
      "Test-Error:0.293 Test-KKR:0.8638 Training-Error:0.126 Training-KKR:0.972\n",
      "Test-Error:0.291 Test-KKR:0.8614 Training-Error:0.091 Training-KKR:0.991\n",
      "Test-Error:0.295 Test-KKR:0.86 Training-Error:0.071 Training-KKR:0.994\n",
      "Test-Error:0.299 Test-KKR:0.8584 Training-Error:0.057 Training-KKR:0.995\n",
      "Test-Error:0.304 Test-KKR:0.8571 Training-Error:0.047 Training-KKR:0.997\n",
      "Test-Error:0.307 Test-KKR:0.8555 Training-Error:0.040 Training-KKR:0.998\n",
      "Test-Error:0.311 Test-KKR:0.8524 Training-Error:0.034 Training-KKR:0.998\n",
      "Test-Error:0.314 Test-KKR:0.8513 Training-Error:0.030 Training-KKR:0.998\n",
      "Test-Error:0.318 Test-KKR:0.8505 Training-Error:0.026 Training-KKR:0.998\n",
      "Test-Error:0.321 Test-KKR:0.8491 Training-Error:0.023 Training-KKR:0.998\n",
      "Test-Error:0.323 Test-KKR:0.8467 Training-Error:0.020 Training-KKR:0.998\n",
      "Test-Error:0.326 Test-KKR:0.8445 Training-Error:0.018 Training-KKR:0.998\n",
      "Test-Error:0.327 Test-KKR:0.8437 Training-Error:0.016 Training-KKR:0.998\n",
      "Test-Error:0.329 Test-KKR:0.8427 Training-Error:0.014 Training-KKR:0.998\n",
      "Test-Error:0.331 Test-KKR:0.8414 Training-Error:0.013 Training-KKR:0.998\n",
      "Test-Error:0.333 Test-KKR:0.8405 Training-Error:0.012 Training-KKR:0.998\n",
      "Test-Error:0.335 Test-KKR:0.839 Training-Error:0.010 Training-KKR:0.998\n",
      "Test-Error:0.337 Test-KKR:0.8377 Training-Error:0.009 Training-KKR:0.999\n",
      "Test-Error:0.338 Test-KKR:0.8368 Training-Error:0.009 Training-KKR:0.999\n",
      "Test-Error:0.340 Test-KKR:0.8365 Training-Error:0.008 Training-KKR:0.999\n",
      "Test-Error:0.341 Test-KKR:0.8356 Training-Error:0.007 Training-KKR:0.999\n",
      "Test-Error:0.343 Test-KKR:0.8346 Training-Error:0.007 Training-KKR:0.999\n",
      "Test-Error:0.344 Test-KKR:0.8344 Training-Error:0.006 Training-KKR:0.999\n",
      "Test-Error:0.345 Test-KKR:0.8336 Training-Error:0.006 Training-KKR:0.999\n",
      "Test-Error:0.347 Test-KKR:0.8342 Training-Error:0.005 Training-KKR:0.999\n",
      "Test-Error:0.348 Test-KKR:0.8332 Training-Error:0.005 Training-KKR:0.999\n",
      "Test-Error:0.349 Test-KKR:0.8326 Training-Error:0.005 Training-KKR:0.999\n",
      "Test-Error:0.350 Test-KKR:0.832 Training-Error:0.004 Training-KKR:0.999\n",
      "Test-Error:0.351 Test-KKR:0.8313 Training-Error:0.004 Training-KKR:0.999\n",
      "Test-Error:0.352 Test-KKR:0.831 Training-Error:0.004 Training-KKR:0.999\n",
      "Test-Error:0.353 Test-KKR:0.83 Training-Error:0.004 Training-KKR:0.999\n",
      "Test-Error:0.354 Test-KKR:0.8294 Training-Error:0.003 Training-KKR:0.999\n",
      "Test-Error:0.354 Test-KKR:0.8293 Training-Error:0.003 Training-KKR:0.999\n",
      "Test-Error:0.355 Test-KKR:0.829 Training-Error:0.003 Training-KKR:0.999\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}